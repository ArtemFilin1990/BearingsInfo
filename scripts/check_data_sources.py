#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ Baza.
Script to check and analyze data sources in the Baza repository.

Usage:
    python scripts/check_data_sources.py
"""

import csv
import os
from pathlib import Path
from typing import Dict, List, Tuple

# Constants for data quality thresholds
SMALL_FILE_THRESHOLD = 10  # Files with fewer records
SMALL_FILE_DISPLAY_THRESHOLD = 50  # Show headers for files with fewer records
LARGE_FILE_THRESHOLD = 50000  # Files that might need splitting


def analyze_csv_structure(file_path: Path) -> Tuple[int, List[str]]:
    """Analyze CSV file structure and return row count and columns."""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            reader = csv.reader(f)
            headers = next(reader)
            row_count = sum(1 for _ in reader)
            return row_count, headers
    except Exception as e:
        print(f"Error analyzing {file_path}: {e}")
        return 0, []


def scan_data_directory(data_dir: Path) -> Dict[str, Dict]:
    """Scan data directory and return statistics."""
    results = {}

    for csv_file in data_dir.rglob("*.csv"):
        rel_path = csv_file.relative_to(data_dir)
        row_count, headers = analyze_csv_structure(csv_file)

        results[str(rel_path)] = {
            "rows": row_count,
            "columns": len(headers),
            "headers": headers,
            "size_kb": csv_file.stat().st_size / 1024,
        }

    return results


def print_statistics(stats: Dict[str, Dict]):
    """Print formatted statistics."""
    print("=" * 80)
    print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ê–ù–ù–´–• / DATA STATISTICS")
    print("=" * 80)
    print()

    # Group by category
    categories = {}
    for file_path, data in stats.items():
        category = file_path.split("/")[0] if "/" in file_path else "root"
        if category not in categories:
            categories[category] = []
        categories[category].append((file_path, data))

    total_rows = 0
    total_files = 0

    for category, files in sorted(categories.items()):
        print(f"\nüìÅ {category.upper()}")
        print("-" * 80)

        category_rows = 0
        for file_path, data in sorted(files):
            print(f"  üìÑ {file_path}")
            print(f"     –°—Ç—Ä–æ–∫/Rows: {data['rows']:,}")
            print(f"     –°—Ç–æ–ª–±—Ü–æ–≤/Columns: {data['columns']}")
            print(f"     –†–∞–∑–º–µ—Ä/Size: {data['size_kb']:.2f} KB")
            if data["rows"] < SMALL_FILE_DISPLAY_THRESHOLD:  # Show headers for small files
                print(f"     –ó–∞–≥–æ–ª–æ–≤–∫–∏: {', '.join(data['headers'][:5])}")
            print()

            category_rows += data["rows"]
            total_rows += data["rows"]
            total_files += 1

        print(f"  –ò—Ç–æ–≥–æ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category_rows:,} —Å—Ç—Ä–æ–∫")

    print("=" * 80)
    print(f"–í–°–ï–ì–û / TOTAL:")
    print(f"  –§–∞–π–ª–æ–≤ / Files: {total_files}")
    print(f"  –ó–∞–ø–∏—Å–µ–π / Records: {total_rows:,}")
    print("=" * 80)


def check_data_quality(stats: Dict[str, Dict]) -> List[str]:
    """Check data quality and return list of recommendations."""
    recommendations = []

    # Check for empty files
    empty_files = [f for f, d in stats.items() if d["rows"] == 0]
    if empty_files:
        recommendations.append(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω—ã –ø—É—Å—Ç—ã–µ —Ñ–∞–π–ª—ã / Empty files found: {', '.join(empty_files)}")

    # Check for small files that might need expansion
    small_files = [f for f, d in stats.items() if 0 < d["rows"] < SMALL_FILE_THRESHOLD]
    if small_files:
        recommendations.append(
            f"üìù –§–∞–π–ª—ã —Å –º–∞–ª—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö (–º–æ–≥—É—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è):\n   " f"{', '.join(small_files)}"
        )

    # Check for very large files that might need splitting
    large_files = [f for f, d in stats.items() if d["rows"] > LARGE_FILE_THRESHOLD]
    if large_files:
        recommendations.append(f"üíæ –ë–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã (–≤–æ–∑–º–æ–∂–Ω–æ, —Å—Ç–æ–∏—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç—å):\n   " f"{', '.join(large_files)}")

    return recommendations


def main():
    """Main function."""
    # Find repository root
    script_dir = Path(__file__).parent
    repo_root = script_dir.parent
    data_dir = repo_root / "data"

    if not data_dir.exists():
        print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {data_dir}")
        return 1

    print(f"üîç –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {data_dir}")
    print()

    # Scan and analyze
    stats = scan_data_directory(data_dir)

    if not stats:
        print("‚ùå CSV —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return 1

    # Print statistics
    print_statistics(stats)

    # Check quality
    print("\nüìã –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò / RECOMMENDATIONS")
    print("=" * 80)
    recommendations = check_data_quality(stats)

    if recommendations:
        for rec in recommendations:
            print(f"\n{rec}")
    else:
        print("\n‚úÖ –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –≤ –Ω–æ—Ä–º–µ / Data quality is good")

    print("\n" + "=" * 80)

    return 0


if __name__ == "__main__":
    exit(main())
