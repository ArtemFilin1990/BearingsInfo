# Bearing Data Processing Pipeline - Automation Documentation

## Обзор

Автоматический конвейер обработки данных по подшипникам. Система автоматически обрабатывает файлы из папки `inbox/`, нормализует данные, обновляет единую таблицу каталога и перемещает обработанные файлы в `processed/` или `error/`.

## Структура репозитория

```
BearingsInfo/
├── inbox/              # Входящие файлы (сюда кладёт пользователь)
├── processed/          # Успешно обработанные файлы
├── error/              # Файлы с ошибками обработки
├── out/                # Результаты и артефакты
│   ├── catalog_target.csv      # Единая таблица каталога (CSV)
│   ├── catalog_target.json     # Единая таблица каталога (JSON)
│   ├── run_report.ndjson       # Отчёт по обработке (NDJSON)
│   └── processed_registry.json # Реестр обработанных файлов
├── logs/               # Логи работы сервиса
│   └── app.log        # Основной лог
├── src/                # Исходный код
│   ├── catalog.py     # Управление каталогом
│   ├── cli.py         # Командная строка
│   ├── config.py      # Конфигурация
│   ├── logger.py      # Логирование и отчёты
│   ├── parser.py      # Парсинг файлов
│   ├── processor.py   # Основной процессор
│   ├── registry.py    # Реестр файлов
│   ├── utils.py       # Утилиты
│   └── watcher.py     # Мониторинг папки
├── config/             # Конфигурация
│   ├── app.yaml               # Настройки приложения
│   ├── brand_aliases.json     # Маппинг брендов
│   └── parsing_rules.json     # Правила парсинга
├── bin/
│   └── run            # Скрипт запуска
└── tests/              # Тесты
    └── test_processor.py
```

## Установка и настройка

### 1. Установка зависимостей

```bash
pip install -r requirements.txt
```

Зависимости:
- Python 3.11+
- pandas >= 2.0.0
- openpyxl >= 3.1.0
- pyyaml >= 6.0.1
- watchdog >= 3.0.0 (опционально, для file watcher)

### 2. Настройка конфигурации

Основные настройки находятся в `config/app.yaml`:

- **paths**: Пути к папкам
- **watcher**: Режим мониторинга (poll/watch) и интервал
- **limits**: Ограничения (макс. размер файла, кол-во строк)
- **normalization**: Правила нормализации данных
- **logging**: Настройки логирования

Маппинг брендов в `config/brand_aliases.json`:
```json
{
  "aliases": {
    "skf": "SKF",
    "fag": "FAG",
    "nsk": "NSK"
  }
}
```

Правила парсинга в `config/parsing_rules.json`:
- Маппинг колонок (альтернативные названия)
- Регулярные выражения для извлечения размеров
- Обязательные поля

## Запуск

### Режим watch (по умолчанию)

Следит за папкой `inbox/` и автоматически обрабатывает новые файлы:

```bash
python -m src.cli watch
```

или через скрипт:

```bash
./bin/run watch
```

Остановка: `Ctrl+C`

### Режим once

Обработать все файлы из `inbox/` один раз и выйти:

```bash
python -m src.cli once
```

### Режим rebuild

Пересобрать каталог из всех файлов в `processed/`:

```bash
python -m src.cli rebuild
```

### Дополнительные параметры

```bash
# Указать директорию конфигурации
python -m src.cli watch --config-dir /path/to/config

# Изменить уровень логирования
python -m src.cli watch --log-level DEBUG
```

## Использование

### 1. Добавление файлов

Просто скопируйте или переместите файл в папку `inbox/`:

```bash
cp my_bearings_data.csv inbox/
```

В режиме **watch** файл будет автоматически обработан.

### 2. Поддерживаемые форматы

- **CSV** (.csv)
- **Excel** (.xlsx, .xls)
- **JSON** (.json) - массив объектов или объект с массивом
- **TXT/MD** (.txt, .md) - табличные данные или строки с размерами

### 3. Структура данных

Целевая схема каталога:

| Колонка       | Тип    | Описание              |
|---------------|--------|-----------------------|
| Наименование  | текст  | Название изделия      |
| Артикул       | текст  | Артикул/обозначение   |
| Аналог        | текст  | Аналог                |
| Бренд         | текст  | Бренд/производитель   |
| D             | число  | Наружный диаметр (мм) |
| d             | число  | Внутренний диаметр    |
| H             | число  | Высота/ширина         |
| m             | число  | Масса (кг)            |

Система автоматически распознает альтернативные названия колонок (см. `config/parsing_rules.json`).

### 4. Обработка результатов

После обработки файл будет:
- Перемещён в `processed/` с нормализованным именем: `YYYYMMDD_HHMMSS__<source>__<n_records>__<sha256_8>.<ext>`
- Или в `error/` при ошибке: `YYYYMMDD_HHMMSS__<source>__0__<sha256>__ERROR__<code>.<ext>`

Результаты доступны в папке `out/`:
- **catalog_target.csv** - единый каталог в CSV
- **catalog_target.json** - единый каталог в JSON
- **run_report.ndjson** - детальный отчёт по каждому файлу

## Ключевые возможности

### Идемпотентность

Система использует SHA256-хеширование для отслеживания обработанных файлов. Один и тот же файл (по содержимому) не будет обработан дважды:

```
1. Файл обрабатывается → добавляется в реестр
2. Тот же файл снова → пропускается (статус: skipped_duplicate)
```

### Нормализация данных

#### Текст
- Убираются лишние пробелы
- Унификация символов (×, х → x; различные дефисы → -)

#### Числа
- Замена запятой на точку
- Преобразование в float

#### Бренды
- Приведение к единому регистру (UPPER или Title Case)
- Маппинг синонимов через `config/brand_aliases.json`

### Дедупликация

Ключ дедупликации:
- Если **Бренд** заполнен: `(Артикул, Бренд, D, d, H)`
- Если **Бренд** пуст: `(Артикул, D, d, H)`

**Конфликты размеров**: Если для одного артикула/бренда встречаются разные размеры:
- Добавляются как отдельные строки
- Фиксируются в отчёте как конфликты

### Логирование и отчётность

#### Логи (`logs/app.log`)
Структурированные JSON-логи с полями:
- timestamp
- level
- message
- file (имя файла)
- sha (хеш)
- status
- n_rows, n_added, n_skipped, n_conflicts

#### Отчёт (`out/run_report.ndjson`)
Каждая строка - JSON-объект с информацией о файле:
```json
{
  "timestamp": "2024-01-23T12:00:00Z",
  "filename": "bearings.csv",
  "sha256": "abc123...",
  "status": "success",
  "n_rows": 100,
  "n_added": 95,
  "n_skipped": 5,
  "n_conflicts": 2,
  "processing_time_sec": 1.234
}
```

## Тестирование

Запуск тестов:

```bash
pytest tests/ -v
```

Запуск конкретного теста:

```bash
pytest tests/test_processor.py::TestProcessor::test_idempotent_processing -v
```

Тесты покрывают:
1. ✅ CSV с понятными колонками → корректный каталог
2. ✅ XLSX с альтернативными названиями колонок → корректный маппинг
3. ✅ Повторная загрузка того же файла → идемпотентность
4. ✅ Конфликт размеров → две строки + запись в отчёт
5. ✅ Невалидный файл → уход в error/ + отчёт

## Устранение неполадок

### Файл не обрабатывается

1. Проверьте логи: `tail -f logs/app.log`
2. Проверьте формат файла (поддерживается ли расширение)
3. Убедитесь, что файл не превышает лимит размера (по умолчанию 50 МБ)
4. Проверьте, что в файле есть обязательные поля (Артикул или Наименование)

### Ошибка "File too large"

Увеличьте лимит в `config/app.yaml`:
```yaml
limits:
  max_file_size_mb: 100
```

### Данные не нормализуются корректно

1. Проверьте маппинг колонок в `config/parsing_rules.json`
2. Добавьте альтернативные названия колонок
3. Проверьте правила нормализации в `config/app.yaml`

### Дубликаты в каталоге

Проверьте логику дедупликации в отчёте `out/run_report.ndjson`. Если записи отличаются хотя бы по одному полю из ключа дедупликации, они будут добавлены как отдельные.

## Безопасность

- ✅ Не исполняется содержимое файлов
- ✅ Ограничение размера входных файлов
- ✅ Атомарная запись (временный файл → rename)
- ✅ Валидация данных перед обработкой

## Производительность

- Polling mode: проверка каждые N секунд (настраивается)
- Watch mode: мгновенная реакция на новые файлы (требует watchdog)
- Обработка файлов последовательная (по одному)

## Примеры использования

### Пример 1: Обработка CSV с подшипниками

Файл `bearings.csv`:
```csv
Артикул,Бренд,d,D,H
6200,SKF,10,30,9
6201,FAG,12,32,10
```

Команда:
```bash
cp bearings.csv inbox/
python -m src.cli once
```

Результат:
- Файл перемещён в `processed/20240123_120000__bearings__2__a1b2c3d4.csv`
- 2 записи добавлены в `out/catalog_target.csv`
- Отчёт в `out/run_report.ndjson`

### Пример 2: Автоматический мониторинг

```bash
# Запустить в фоне
python -m src.cli watch &

# Добавить файлы
cp file1.xlsx inbox/
cp file2.csv inbox/

# Файлы автоматически обработаются
```

### Пример 3: Восстановление каталога

```bash
# Удалить текущий каталог
rm out/catalog_target.*

# Пересобрать из обработанных файлов
python -m src.cli rebuild
```

## FAQ

**Q: Можно ли обрабатывать файлы параллельно?**  
A: Текущая реализация обрабатывает файлы последовательно для обеспечения целостности данных.

**Q: Что происходит при перезапуске?**  
A: Реестр обработанных файлов сохраняется в `out/processed_registry.json`, поэтому файлы не будут обработаны повторно.

**Q: Как добавить новый формат файла?**  
A: Расширьте класс `DataParser` в `src/parser.py` и добавьте метод для нового формата.

**Q: Как изменить схему каталога?**  
A: Измените `TARGET_COLUMNS` в `src/catalog.py` и обновите маппинги в `config/parsing_rules.json`.

## Контакты и поддержка

Для вопросов и предложений создавайте issue в репозитории GitHub.
