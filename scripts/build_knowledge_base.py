#!/usr/bin/env python3
"""
Knowledge Base Builder (MAX CONTEXT MODE)

–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–π, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏ –º–∞—à–∏–Ω–æ—á–∏—Ç–∞–µ–º–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
–Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏.

–†–µ–∂–∏–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ - –Ω–∏–∫–∞–∫–∏—Ö —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π, –ø–æ–ª–Ω–∞—è –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.
"""

import os
import json
import re
from pathlib import Path
from typing import Dict, List, Set, Tuple, Any
from collections import defaultdict
from datetime import datetime
import mimetypes


class KnowledgeBaseBuilder:
    """
    –ü–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.
    
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç 100% –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Ñ–∞–π–ª–æ–≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç
    —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown.
    """
    
    def __init__(self, repo_path: str, output_path: str = "KNOWLEDGE_BASE.md"):
        self.repo_path = Path(repo_path)
        self.output_path = Path(output_path)
        
        # –ò—Å–∫–ª—é—á–∞–µ–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏ —Ñ–∞–π–ª—ã
        self.exclude_dirs = {
            '.git', '__pycache__', '.pytest_cache', 'node_modules',
            '.venv', 'venv', 'dist', 'build', '.mypy_cache',
            '_trash_review', 'ZIP'
        }
        
        self.exclude_patterns = {
            '.pyc', '.pyo', '.pyd', '.so', '.dll', '.dylib',
            '.exe', '.bin', '.o', '.a', '.class', '.jar',
            '.gitignore', '.gitattributes', '.DS_Store'
        }
        
        # –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
        self.file_inventory: List[Dict[str, Any]] = []
        self.terms_glossary: Dict[str, List[str]] = defaultdict(list)
        self.processes: Dict[str, List[str]] = defaultdict(list)
        self.rules: Dict[str, List[str]] = defaultdict(list)
        self.data_structures: Dict[str, List[str]] = defaultdict(list)
        self.conflicts: List[Dict[str, Any]] = []
        
    def should_process_file(self, file_path: Path) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ñ–∞–π–ª."""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        for part in file_path.parts:
            if part in self.exclude_dirs:
                return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π
        if any(str(file_path).endswith(pattern) for pattern in self.exclude_patterns):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞ (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–µ –±–∏–Ω–∞—Ä–Ω—ã–µ —Ñ–∞–π–ª—ã)
        try:
            if file_path.stat().st_size > 100 * 1024 * 1024:  # 100 MB
                return False
        except:
            return False
        
        return True
    
    def get_file_type(self, file_path: Path) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —Ñ–∞–π–ª–∞."""
        suffix = file_path.suffix.lower()
        
        type_mapping = {
            '.md': 'Markdown',
            '.txt': 'Text',
            '.py': 'Python',
            '.json': 'JSON',
            '.yaml': 'YAML',
            '.yml': 'YAML',
            '.csv': 'CSV',
            '.xlsx': 'Excel',
            '.xls': 'Excel',
            '.pdf': 'PDF',
            '.docx': 'Word',
            '.doc': 'Word',
            '.html': 'HTML',
            '.xml': 'XML',
            '.sql': 'SQL',
            '.sh': 'Shell',
            '.bat': 'Batch',
            '.toml': 'TOML',
            '.ini': 'INI',
            '.cfg': 'Config',
            '.conf': 'Config',
            '.js': 'JavaScript',
            '.ts': 'TypeScript',
            '.jsx': 'React',
            '.tsx': 'React TypeScript',
            '.css': 'CSS',
            '.scss': 'SCSS',
            '.png': 'Image (PNG)',
            '.jpg': 'Image (JPEG)',
            '.jpeg': 'Image (JPEG)',
            '.gif': 'Image (GIF)',
            '.svg': 'Image (SVG)',
            '.zip': 'Archive (ZIP)',
            '.tar': 'Archive (TAR)',
            '.gz': 'Archive (GZ)',
        }
        
        return type_mapping.get(suffix, f'Other ({suffix})')
    
    def read_file_content(self, file_path: Path) -> str:
        """–ß–∏—Ç–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞."""
        try:
            # –ü–æ–ø—ã—Ç–∫–∞ –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except UnicodeDecodeError:
            try:
                # –ü–æ–ø—ã—Ç–∫–∞ —Å –¥—Ä—É–≥–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
                with open(file_path, 'r', encoding='cp1251') as f:
                    return f.read()
            except:
                pass
        except Exception:
            pass
        
        # –î–ª—è –±–∏–Ω–∞—Ä–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        try:
            size = file_path.stat().st_size
            return f"[[BINARY FILE: {size} bytes]]"
        except:
            return "[[DATA NOT FOUND]]"
    
    def extract_terms_from_markdown(self, content: str, file_path: Path) -> List[Tuple[str, str]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ Markdown —Ñ–∞–π–ª–∞."""
        terms = []
        
        # –ü–æ–∏—Å–∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ "**–¢–µ—Ä–º–∏–Ω** - –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ"
        pattern1 = r'\*\*([^*]+)\*\*\s*[-‚Äî‚Äì]\s*([^\n]+)'
        for match in re.finditer(pattern1, content):
            term = match.group(1).strip()
            definition = match.group(2).strip()
            terms.append((term, definition))
        
        # –ü–æ–∏—Å–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∫–∞–∫ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        pattern2 = r'^#+\s+(.+)$'
        for match in re.finditer(pattern2, content, re.MULTILINE):
            heading = match.group(1).strip()
            terms.append((heading, f"–†–∞–∑–¥–µ–ª: {heading}"))
        
        return terms
    
    def extract_code_structures(self, content: str, file_type: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫–æ–¥–∞."""
        structures = []
        
        if file_type == 'Python':
            # –ö–ª–∞—Å—Å—ã
            class_pattern = r'class\s+(\w+).*?:'
            structures.extend([f"Class: {m.group(1)}" for m in re.finditer(class_pattern, content)])
            
            # –§—É–Ω–∫—Ü–∏–∏
            func_pattern = r'def\s+(\w+)\s*\('
            structures.extend([f"Function: {m.group(1)}" for m in re.finditer(func_pattern, content)])
        
        elif file_type == 'JSON':
            try:
                data = json.loads(content)
                structures.append(f"JSON structure with {len(data)} top-level keys" if isinstance(data, dict) else f"JSON array with {len(data)} items")
            except:
                pass
        
        return structures
    
    def analyze_file(self, file_path: Path) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª."""
        file_type = self.get_file_type(file_path)
        content = self.read_file_content(file_path)
        
        # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –æ—Ç –∫–æ—Ä–Ω—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
        rel_path = file_path.relative_to(self.repo_path)
        
        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ
        file_info = {
            'name': file_path.name,
            'path': str(rel_path),
            'type': file_type,
            'size': file_path.stat().st_size if file_path.exists() else 0,
            'purpose': self.infer_purpose(file_path, content),
            'key_topics': self.extract_key_topics(content, file_type),
            'content_preview': content[:500] if content and not content.startswith('[[') else content,
        }
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ Markdown
        if file_type == 'Markdown':
            terms = self.extract_terms_from_markdown(content, file_path)
            for term, definition in terms:
                self.terms_glossary[term].append(f"{definition} (–∏—Å—Ç–æ—á–Ω–∏–∫: {rel_path})")
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä –∏–∑ –∫–æ–¥–∞
        if file_type in ['Python', 'JSON', 'JavaScript']:
            structures = self.extract_code_structures(content, file_type)
            for struct in structures:
                self.data_structures[str(rel_path)].append(struct)
        
        return file_info
    
    def infer_purpose(self, file_path: Path, content: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ —Ñ–∞–π–ª–∞."""
        name_lower = file_path.name.lower()
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã
        if name_lower in ['readme.md', 'readme.txt']:
            return '–û—Å–Ω–æ–≤–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞'
        elif name_lower == 'license':
            return '–õ–∏—Ü–µ–Ω–∑–∏—è –ø—Ä–æ–µ–∫—Ç–∞'
        elif name_lower in ['changelog.md', 'changelog.txt']:
            return '–ò—Å—Ç–æ—Ä–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π'
        elif name_lower in ['contributing.md', 'contributing.txt']:
            return '–†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ —É—á–∞—Å—Ç–∏—é –≤ –ø—Ä–æ–µ–∫—Ç–µ'
        elif 'test' in name_lower:
            return '–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ'
        elif 'config' in name_lower or 'setup' in name_lower:
            return '–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è'
        
        # –ü–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        parts = file_path.parts
        if 'docs' in parts or 'documentation' in parts:
            return '–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è'
        elif 'scripts' in parts:
            return '–°–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏'
        elif 'tests' in parts:
            return '–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ'
        elif 'api' in parts:
            return 'API'
        elif 'src' in parts or 'source' in parts:
            return '–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥'
        
        # –ü–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É (–ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏)
        if content and not content.startswith('[['):
            first_lines = content[:200].lower()
            if 'class' in first_lines or 'def ' in first_lines:
                return '–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏ –∫–ª–∞—Å—Å–æ–≤/—Ñ—É–Ω–∫—Ü–∏–π'
            elif '#' in first_lines and content.startswith('#'):
                return '–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∏–ª–∏ —Å–ø—Ä–∞–≤–æ—á–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª'
        
        return '–û–±—â–∏–π —Ñ–∞–π–ª'
    
    def extract_key_topics(self, content: str, file_type: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã –∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ."""
        topics = []
        
        if not content or content.startswith('[['):
            return topics
        
        # –î–ª—è Markdown - –∑–∞–≥–æ–ª–æ–≤–∫–∏
        if file_type == 'Markdown':
            headings = re.findall(r'^#+\s+(.+)$', content, re.MULTILINE)
            topics.extend(headings[:5])  # –ü–µ—Ä–≤—ã–µ 5 –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        
        # –û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ —á–∞—Å—Ç–æ—Ç–µ)
        words = re.findall(r'\b[–∞-—è–ê-–Ø—ë–Åa-zA-Z]{4,}\b', content)
        word_freq = defaultdict(int)
        for word in words:
            word_freq[word.lower()] += 1
        
        # –¢–æ–ø-5 —Å–ª–æ–≤
        top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]
        topics.extend([word for word, _ in top_words])
        
        return list(set(topics))[:10]  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ, –º–∞–∫—Å–∏–º—É–º 10
    
    def scan_repository(self):
        """–°–∫–∞–Ω–∏—Ä—É–µ—Ç –≤–µ—Å—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π."""
        print("üîç –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è...")
        
        for root, dirs, files in os.walk(self.repo_path):
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
            dirs[:] = [d for d in dirs if d not in self.exclude_dirs]
            
            root_path = Path(root)
            
            for file in files:
                file_path = root_path / file
                
                if self.should_process_file(file_path):
                    try:
                        file_info = self.analyze_file(file_path)
                        self.file_inventory.append(file_info)
                        
                        if len(self.file_inventory) % 100 == 0:
                            print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(self.file_inventory)}")
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_path}: {e}")
        
        print(f"‚úÖ –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(self.file_inventory)}")
    
    def generate_inventory_table(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–∞–±–ª–∏—Ü—É –∏–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü–∏–∏ —Ñ–∞–π–ª–æ–≤."""
        lines = [
            "## –®–∞–≥ 1. –ò–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤",
            "",
            "–ü–æ–ª–Ω—ã–π –ø–µ—Ä–µ—á–µ–Ω—å –≤—Å–µ—Ö –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏.",
            "",
            "| ‚Ññ | –ò–º—è —Ñ–∞–π–ª–∞ | –¢–∏–ø | –†–∞–∑–º–µ—Ä (–±–∞–π—Ç) | –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ | –ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã |",
            "|---|-----------|-----|---------------|------------|---------------|"
        ]
        
        for idx, file_info in enumerate(self.file_inventory, 1):
            topics = ', '.join(file_info['key_topics'][:3]) if file_info['key_topics'] else '-'
            lines.append(
                f"| {idx} | `{file_info['name']}` | {file_info['type']} | "
                f"{file_info['size']} | {file_info['purpose']} | {topics} |"
            )
        
        lines.extend([
            "",
            f"**–ò—Ç–æ–≥–æ —Ñ–∞–π–ª–æ–≤:** {len(self.file_inventory)}",
            ""
        ])
        
        return '\n'.join(lines)
    
    def generate_file_details(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞."""
        lines = [
            "## –®–∞–≥ 2. –î–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ",
            "",
            "–ü–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞ —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤.",
            ""
        ]
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–∏–ø–∞–º
        files_by_type = defaultdict(list)
        for file_info in self.file_inventory:
            files_by_type[file_info['type']].append(file_info)
        
        for file_type in sorted(files_by_type.keys()):
            lines.append(f"### –§–∞–π–ª—ã —Ç–∏–ø–∞: {file_type}")
            lines.append("")
            
            for file_info in files_by_type[file_type]:
                lines.append(f"#### `{file_info['path']}`")
                lines.append("")
                lines.append(f"- **–ò–º—è:** {file_info['name']}")
                lines.append(f"- **–¢–∏–ø:** {file_info['type']}")
                lines.append(f"- **–†–∞–∑–º–µ—Ä:** {file_info['size']} –±–∞–π—Ç")
                lines.append(f"- **–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** {file_info['purpose']}")
                
                if file_info['key_topics']:
                    lines.append(f"- **–ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã:** {', '.join(file_info['key_topics'])}")
                
                if file_info['content_preview'] and not file_info['content_preview'].startswith('[['):
                    lines.append("")
                    lines.append("**–ü—Ä–µ–≤—å—é —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ:**")
                    lines.append("```")
                    lines.append(file_info['content_preview'])
                    lines.append("```")
                
                lines.append("")
        
        return '\n'.join(lines)
    
    def generate_glossary(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥–ª–æ—Å—Å–∞—Ä–∏–π —Ç–µ—Ä–º–∏–Ω–æ–≤."""
        lines = [
            "## 2. –¢–µ—Ä–º–∏–Ω—ã –∏ –≥–ª–æ—Å—Å–∞—Ä–∏–π",
            "",
            "–í—Å–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–æ–≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è.",
            "",
            "| –¢–µ—Ä–º–∏–Ω | –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è | –ò—Å—Ç–æ—á–Ω–∏–∫–∏ |",
            "|--------|-------------|-----------|"
        ]
        
        for term in sorted(self.terms_glossary.keys()):
            definitions = self.terms_glossary[term]
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, —É–∫–∞–∑—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            first_def = definitions[0] if definitions else "[[DATA NOT FOUND]]"
            source_count = len(definitions)
            
            lines.append(
                f"| {term} | {first_def[:100]}... | {source_count} –∏—Å—Ç–æ—á–Ω–∏–∫(–æ–≤) |"
            )
        
        lines.append("")
        lines.append(f"**–í—Å–µ–≥–æ —Ç–µ—Ä–º–∏–Ω–æ–≤:** {len(self.terms_glossary)}")
        lines.append("")
        
        return '\n'.join(lines)
    
    def generate_data_structures_section(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–∞–∑–¥–µ–ª —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö."""
        lines = [
            "## 5. –°—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Ñ–æ—Ä–º–∞—Ç—ã",
            "",
            "–°—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –∫–ª–∞—Å—Å—ã, —Ñ—É–Ω–∫—Ü–∏–∏ –∏ —Ñ–æ—Ä–º–∞—Ç—ã, –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –≤ –∫–æ–¥–µ.",
            ""
        ]
        
        if not self.data_structures:
            lines.append("[[DATA NOT FOUND]]")
            lines.append("")
            return '\n'.join(lines)
        
        for file_path in sorted(self.data_structures.keys()):
            structures = self.data_structures[file_path]
            if structures:
                lines.append(f"### –§–∞–π–ª: `{file_path}`")
                lines.append("")
                for struct in structures:
                    lines.append(f"- {struct}")
                lines.append("")
        
        return '\n'.join(lines)
    
    def generate_knowledge_base(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π."""
        print("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
        
        lines = [
            "# –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π: BearingsInfo",
            "",
            f"**–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "–ò—Å—á–µ—Ä–ø—ã–≤–∞—é—â–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è.",
            "–†–µ–∂–∏–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ - –ø–æ–ª–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –±–µ–∑ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π.",
            "",
            "---",
            "",
            "# –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ",
            "",
            "1. [–ò–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤](#—à–∞–≥-1-–∏–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü–∏—è-—Ñ–∞–π–ª–æ–≤)",
            "2. [–¢–µ—Ä–º–∏–Ω—ã –∏ –≥–ª–æ—Å—Å–∞—Ä–∏–π](#2-—Ç–µ—Ä–º–∏–Ω—ã-–∏-–≥–ª–æ—Å—Å–∞—Ä–∏–π)",
            "3. [–ü—Ä–æ—Ü–µ—Å—Å—ã –∏ –∞–ª–≥–æ—Ä–∏—Ç–º—ã](#3-–ø—Ä–æ—Ü–µ—Å—Å—ã-–∏-–∞–ª–≥–æ—Ä–∏—Ç–º—ã)",
            "4. [–ü—Ä–∞–≤–∏–ª–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è](#4-–ø—Ä–∞–≤–∏–ª–∞-–∏-–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è)",
            "5. [–°—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Ñ–æ—Ä–º–∞—Ç—ã](#5-—Å—Ç—Ä—É–∫—Ç—É—Ä—ã-–¥–∞–Ω–Ω—ã—Ö-–∏-—Ñ–æ—Ä–º–∞—Ç—ã)",
            "6. [–†–æ–ª–∏ –∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏](#6-—Ä–æ–ª–∏-–∏-–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏)",
            "7. [–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è](#7-–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏-–∏-—Å—Ü–µ–Ω–∞—Ä–∏–∏-–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)",
            "8. [–û—à–∏–±–∫–∏, –∏—Å–∫–ª—é—á–µ–Ω–∏—è, –∫—Ä–∞–π–Ω–∏–µ —Å–ª—É—á–∞–∏](#8-–æ—à–∏–±–∫–∏-–∏—Å–∫–ª—é—á–µ–Ω–∏—è-–∫—Ä–∞–π–Ω–∏–µ-—Å–ª—É—á–∞–∏)",
            "9. [–°–≤—è–∑–∏ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏](#9-—Å–≤—è–∑–∏-–∏-–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏-–º–µ–∂–¥—É-—Å—É—â–Ω–æ—Å—Ç—è–º–∏)",
            "10. [–ò—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞](#10-–∏—Å—Ç–æ—á–Ω–∏–∫–∏-–∏-—Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞)",
            "",
            "---",
            ""
        ]
        
        # 1. –ò–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü–∏—è
        lines.append(self.generate_inventory_table())
        
        # 2. –¢–µ—Ä–º–∏–Ω—ã –∏ –≥–ª–æ—Å—Å–∞—Ä–∏–π
        lines.append(self.generate_glossary())
        
        # 3. –ü—Ä–æ—Ü–µ—Å—Å—ã –∏ –∞–ª–≥–æ—Ä–∏—Ç–º—ã
        lines.extend([
            "## 3. –ü—Ä–æ—Ü–µ—Å—Å—ã –∏ –∞–ª–≥–æ—Ä–∏—Ç–º—ã",
            "",
            "–û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤, –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–µ–π—Å—Ç–≤–∏–π.",
            "",
            "[[DATA NOT FOUND - —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–æ–≤]]",
            ""
        ])
        
        # 4. –ü—Ä–∞–≤–∏–ª–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
        lines.extend([
            "## 4. –ü—Ä–∞–≤–∏–ª–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è",
            "",
            "–ü—Ä–∞–≤–∏–ª–∞, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è, –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –∫–æ–¥–µ.",
            "",
            "[[DATA NOT FOUND - —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–æ–≤]]",
            ""
        ])
        
        # 5. –°—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
        lines.append(self.generate_data_structures_section())
        
        # 6-10. –û—Å—Ç–∞–ª—å–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã
        sections = [
            ("6. –†–æ–ª–∏ –∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏", "–û–ø–∏—Å–∞–Ω–∏–µ —Ä–æ–ª–µ–π –∏ –∑–æ–Ω –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –ø—Ä–æ–µ–∫—Ç–µ."),
            ("7. –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è", "–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —Å–∏—Å—Ç–µ–º—ã –∏ —Ç–∏–ø–æ–≤—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏."),
            ("8. –û—à–∏–±–∫–∏, –∏—Å–∫–ª—é—á–µ–Ω–∏—è, –∫—Ä–∞–π–Ω–∏–µ —Å–ª—É—á–∞–∏", "–ò–∑–≤–µ—Å—Ç–Ω—ã–µ –æ—à–∏–±–∫–∏, –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫—Ä–∞–π–Ω–∏—Ö —Å–ª—É—á–∞–µ–≤."),
            ("9. –°–≤—è–∑–∏ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏", "–í–∑–∞–∏–º–æ—Å–≤—è–∑–∏ –º–µ–∂–¥—É –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏, –º–æ–¥—É–ª—è–º–∏ –∏ —Å—É—â–Ω–æ—Å—Ç—è–º–∏."),
            ("10. –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞", "–ü–æ–ª–Ω–∞—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ —Ñ–∞–∫—Ç–æ–≤ –∫ –∏—Å—Ö–æ–¥–Ω—ã–º —Ñ–∞–π–ª–∞–º."),
        ]
        
        for title, description in sections:
            lines.extend([
                f"## {title}",
                "",
                description,
                "",
                "[[DATA NOT FOUND - —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–æ–≤]]",
                ""
            ])
        
        # –î–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ (–¥–µ—Ç–∞–ª—å–Ω–∞—è)
        lines.append("---")
        lines.append("")
        lines.append(self.generate_file_details())
        
        # –ó–∞–ø–∏—Å—å –≤ —Ñ–∞–π–ª
        output_file = self.repo_path / self.output_path
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))
        
        print(f"‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_file}")
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   - –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {len(self.file_inventory)}")
        print(f"   - –¢–µ—Ä–º–∏–Ω–æ–≤ –≤ –≥–ª–æ—Å—Å–∞—Ä–∏–∏: {len(self.terms_glossary)}")
        print(f"   - –°—Ç—Ä—É–∫—Ç—É—Ä –¥–∞–Ω–Ω—ã—Ö: {sum(len(v) for v in self.data_structures.values())}")
    
    def build(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π."""
        print("=" * 70)
        print("  Knowledge Base Builder (MAX CONTEXT MODE)")
        print("  –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è BearingsInfo")
        print("=" * 70)
        print()
        
        # –®–∞–≥ 1: –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.scan_repository()
        
        # –®–∞–≥ 2-4: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        self.generate_knowledge_base()
        
        print()
        print("=" * 70)
        print("  ‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —É—Å–ø–µ—à–Ω–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞!")
        print("=" * 70)


def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Knowledge Base Builder - –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è'
    )
    parser.add_argument(
        '--repo',
        default='.',
        help='–ü—É—Ç—å –∫ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—é (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: —Ç–µ–∫—É—â–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è)'
    )
    parser.add_argument(
        '--output',
        default='KNOWLEDGE_BASE.md',
        help='–ò–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: KNOWLEDGE_BASE.md)'
    )
    
    args = parser.parse_args()
    
    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
    builder = KnowledgeBaseBuilder(args.repo, args.output)
    builder.build()


if __name__ == '__main__':
    main()
